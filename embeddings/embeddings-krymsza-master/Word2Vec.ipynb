{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJsGu7d2AcPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btcjnvBdj50N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4FFduqaPZQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_context_vector(context, word_to_id):\n",
        "    idxs = [word_to_id[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHTzrhOpPZO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_index_of_max(input):\n",
        "    index = 0\n",
        "    for i in range(1, len(input)):\n",
        "        if input[i] > input[index]:\n",
        "            index = i \n",
        "    return index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro_B0OXsPZNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_prob_result(input, id_to_word):\n",
        "    return id_to_word[get_index_of_max(input)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BVxCCqrPZLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "EMDEDDING_DIM = 100\n",
        "\n",
        "word_to_id = {}\n",
        "id_to_word = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxyvahWpPZHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text = \"\"\"We are bout to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT2qOOHYPZFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAtyrwPaPZBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, word in enumerate(vocab):\n",
        "    word_to_id[word] = i\n",
        "    id_to_word[i] = word\n",
        "\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXNriRlwPzrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        #out: 1 x emdedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "        \n",
        "        #out: 1 x vocab_size\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "        word = torch.LongTensor([word_to_id[word]])\n",
        "        return self.embeddings(word).view(1,-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NaV2Fu_P1ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnOIuer8P4FN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        context_vector = make_context_vector(context, word_to_id)  \n",
        "        model.zero_grad()\n",
        "        log_probs = model(context_vector)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_id[target]], dtype=torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3eXAu-6kBKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# TEST\n",
        "context = ['People','create','to', 'direct']\n",
        "context_vector = make_context_vector(context, word_to_id)\n",
        "a = model(context_vector).data.numpy()\n",
        "print('Raw text: {}\\n'.format(' '.join(raw_text)))\n",
        "print('Context: {}\\n'.format(context))\n",
        "print('Prediction: {}'.format(get_max_prob_result(a[0], id_to_word)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9HKKRXWQEVn",
        "colab_type": "text"
      },
      "source": [
        "### Zadania\n",
        "\n",
        "1. Wyrysować wykresy straty, pobawić się parametrami\n",
        "2. Zmienić dane tekstowe na polski\n",
        "3. Jeśli ktoś czuje się na siłach, zaimplementować model skip-gramowy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNLaqfI7QBdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}