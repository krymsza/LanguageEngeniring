{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iA0t2uBWaW9"
      },
      "source": [
        "#Linking Humor and Offense Across Different Age Groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaYIpNdIL8vf"
      },
      "source": [
        "#Notka odnośnie rozpoznawania zdań niezabawnych\n",
        "dane wejściowe:\n",
        "\n",
        "    Is the intention of this text to be humorous? (0 or 1)\n",
        "    [If it is intended to be humorous] How humorous do you find it? (1-5)\n",
        "=> dla zdań nieśmiesznch nie mamy podanej wartości humor_rating\n",
        "\n",
        "np.\n",
        "\n",
        "```\n",
        "id\t                                                            text\tis_humor\thumor_rating\thumor_controversy\toffense_rating\n",
        "7936\tMy girlfriend dumped me on a fishing trip. She left me reeling.\t1\t2.25\t0.0\t0.0\n",
        "4609\t\"Even though I've tried, spaces between us, hold all our secrets, leaving us speechless.' - Spaces.\t0    0.0\n",
        "2940\tOk. Who needs a hug? Anyone? I'm giving some away rn for free!\t0  0.0\n",
        "\n",
        "```\n",
        "\n",
        "wnik dla zdania określonego jako niezabawne:\n",
        "```\n",
        "id                                                 text \tpred \thumor_rating \tis_humor\n",
        "3     There are people out their happier with less t...     0.000607     0.002430     False\n",
        "4     One zebra says to the other, \"I'm going to che...     0.996539     3.986156     True\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9m7Ib-yWaHo"
      },
      "source": [
        "###Przygotowanie środowiska"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7QJLVEnMDdn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow==2.3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2PBf-KLMDd2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "from math import floor, ceil\n",
        "\n",
        "import transformers\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "print(tf.__version__) # >=2.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PzkPXL6MDd8"
      },
      "source": [
        "###Tokenizer \n",
        "\n",
        "`MAX_SEQUENCE_LENGTH 200`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9M9eAbLUvAN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZMizr6DTw2J"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "MAX_SIZE = 200\n",
        "BATCH_SIZE = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVkfk20BTmn-"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqRNgZPyMDd_"
      },
      "outputs": [],
      "source": [
        "HAS_ANS = False\n",
        "training_sample_count = 1000\n",
        "training_epochs = 2\n",
        "test_count = 1000\n",
        "running_folds = 1\n",
        "MAX_SEQUENCE_LENGTH = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5sgk2fjMDeD"
      },
      "source": [
        "### Pobranie datasetu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubrkk7xfNuXj"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def download_and_save(file_name, file_id):\n",
        "  downloaded = drive.CreateFile({'id': file_id})\n",
        "  # fetch file\n",
        "  downloaded.FetchContent()\n",
        "  # write file\n",
        "  with open(file_name,'wb') as f:\n",
        "       f.write(downloaded.content.read())\n",
        "      \n",
        "  print(f'Saved {file_name}')\n",
        "  \n",
        "FILE_NAME, FILE_ID = ['file_name', 'file_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAXdjLXVOD3F"
      },
      "outputs": [],
      "source": [
        "#downloading file \n",
        "#https://drive.google.com/file/d/1xMJBvCPvQD8oWjNBAFEgNhQlLvJ7iuEx/view?usp=sharing\n",
        "file = {FILE_NAME: 'humor_train.csv', FILE_ID: '1xMJBvCPvQD8oWjNBAFEgNhQlLvJ7iuEx'}\n",
        "dataset_path = Path('/content')\n",
        "file_path = dataset_path / file[FILE_NAME]\n",
        "download_and_save(file_path, file[FILE_ID])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHC0YsJePn9n"
      },
      "outputs": [],
      "source": [
        "#downloading file \n",
        "#https://drive.google.com/file/d/18SF4OLtmNvL__BfriC5OJsspQtPJqIVH/view?usp=sharing\n",
        "file = {FILE_NAME: 'humor_test.csv', FILE_ID: '18SF4OLtmNvL__BfriC5OJsspQtPJqIVH'}\n",
        "dataset_path = Path('/content')\n",
        "file_path = dataset_path / file[FILE_NAME]\n",
        "download_and_save(file_path, file[FILE_ID])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaFZwvsJMDeF"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4xmFz5d9T2K"
      },
      "source": [
        "###Formatowanie danych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWtMD5FWMDeH"
      },
      "outputs": [],
      "source": [
        "training_set = pd.read_csv('humor_train.csv')\n",
        "training_set = training_set[['text','is_humor']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb6nzsdeMDeJ"
      },
      "outputs": [],
      "source": [
        "training_set_copy = training_set \n",
        "testing_set = pd.read_csv(\"humor_test.csv\")\n",
        "testing_set = testing_set[['text']]\n",
        "\n",
        "print(\"training_set_copy:\")\n",
        "print(training_set_copy.head())\n",
        "print(\"\\ntesting_set:\")\n",
        "print(testing_set.head())\n",
        "\n",
        "#zmiana rozmiaru\n",
        "testing_set = testing_set[:test_count]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7yGqCIXqdJD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGbZPMX5MDeS"
      },
      "outputs": [],
      "source": [
        "output_categories = list(training_set_copy.columns[[1]])\n",
        "input_categories = list(training_set_copy.columns[[0]])\n",
        "\n",
        "if HAS_ANS:\n",
        "    output_categories = list(training_set_copy.columns[11:])\n",
        "    input_categories = list(training_set_copy.columns[[1,2,5]])\n",
        "    \n",
        "\n",
        "TARGET_COUNT = len(output_categories)\n",
        "print(HAS_ANS)\n",
        "print('input categories:', input_categories)\n",
        "print('output TARGET_COUNT:', TARGET_COUNT)\n",
        "print('output categories:', output_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M_Pd8csEN7u"
      },
      "source": [
        "####Przygotowanie warstw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcd2R3OtMDeU"
      },
      "outputs": [],
      "source": [
        "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \n",
        "      def return_id(str1, str2, truncation_strategy, length):\n",
        "\n",
        "          inputs = tokenizer.encode_plus(str1, str2,\n",
        "              add_special_tokens=True,\n",
        "              max_length=length,\n",
        "              truncation_strategy=truncation_strategy)\n",
        "          \n",
        "          input_ids =  inputs[\"input_ids\"]\n",
        "          input_segments = inputs[\"token_type_ids\"]\n",
        "          input_masks = [1] * len(input_ids)\n",
        "          padding_length = length - len(input_ids)\n",
        "          padding_id = tokenizer.pad_token_id\n",
        "          input_masks = input_masks + ([0] * padding_length)\n",
        "          input_segments = input_segments + ([0] * padding_length)\n",
        "          input_ids = input_ids + ([padding_id] * padding_length)\n",
        "          \n",
        "          \n",
        "          return [input_ids, input_masks, input_segments]\n",
        "      \n",
        "      input_ids_q, input_masks_q, input_segments_q = return_id(title, None, 'longest_first', max_sequence_length)\n",
        "      input_ids_a, input_masks_a, input_segments_a = return_id('', None, 'longest_first', max_sequence_length)\n",
        "          \n",
        "      return [input_ids_q, input_masks_q, input_segments_q,input_ids_a, input_masks_a, input_segments_a]\n",
        "\n",
        "\n",
        "\n",
        "def compute_input_arrays(training_set, columns, tokenizer, max_sequence_length):\n",
        "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
        "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
        "    for _, instance in tqdm(training_set[columns].iterrows()):\n",
        "        t, q, a = instance.text, instance.text, instance.text\n",
        "\n",
        "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        \n",
        "        input_ids_q.append(ids_q)\n",
        "        input_masks_q.append(masks_q)\n",
        "        input_segments_q.append(segments_q)\n",
        "        input_ids_a.append(ids_a)\n",
        "        input_masks_a.append(masks_a)\n",
        "        input_segments_a.append(segments_a)\n",
        "        \n",
        "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
        "            np.asarray(input_masks_q, dtype=np.int32), \n",
        "            np.asarray(input_segments_q, dtype=np.int32),\n",
        "            np.asarray(input_ids_a, dtype=np.int32), \n",
        "            np.asarray(input_masks_a, dtype=np.int32), \n",
        "            np.asarray(input_segments_a, dtype=np.int32)]\n",
        "\n",
        "def compute_output_arrays(training_set, columns):\n",
        "    return np.asarray(training_set[columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDO7mdQpMDeV"
      },
      "outputs": [],
      "source": [
        "outputs = compute_output_arrays(training_set_copy, output_categories)\n",
        "inputs = compute_input_arrays(training_set_copy, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "test_inputs = compute_input_arrays(testing_set, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mivOIXJ7MDeX"
      },
      "source": [
        "###model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yezP5QjT5big"
      },
      "outputs": [],
      "source": [
        "tf.version.VERSION #2.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEin5D8nUR1j"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "#  https://keras.io/api/layers\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qruX1p9-MDeX"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from transformers import BertModel, BertConfig\n",
        "from transformers import TFAutoModel,TFBertModel\n",
        "\n",
        "def create_model(input_shape):\n",
        "    model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "    layer = model.layers[0]\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape, dtype='int32')\n",
        "    input_masks = keras.Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "    outputs = layer([inputs, input_masks])\n",
        "    output = outputs[0]\n",
        "    pooled_output = output[:, 0, :]\n",
        "\n",
        "    is_humor = layers.Dropout(0.3)(pooled_output)\n",
        "    is_humor = layers.Dense(1, activation=\"sigmoid\")(is_humor)\n",
        "\n",
        "    model = keras.Model(inputs=[inputs,input_masks], outputs=is_humor)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE8Vgt2qMDeZ"
      },
      "source": [
        "## Trening, walidacja, testowanie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fMtoBJ_MDeZ"
      },
      "outputs": [],
      "source": [
        "def print_evaluation_metrics(y_true, y_pred, label='', is_regression=True, label2=''):\n",
        "    if is_regression:\n",
        "        return sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
        "    else:\n",
        "        matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
        "        TP,TN,FP,FN = matrix[1][1],matrix[0][0],matrix[0][1],matrix[1][0]\n",
        "        Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
        "        Precision = TP/(TP+FP)\n",
        "        Recall = TP/(TP+FN)\n",
        "        F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
        "        print('Acc', Accuracy, 'Prec', Precision, 'Rec', Recall, 'F1',F1)\n",
        "        return sklearn.metrics.accuracy_score(y_true, y_pred)\n",
        "\n",
        "print_evaluation_metrics([1,0], [0.9,0.1], '', True)\n",
        "print_evaluation_metrics([1,0], [1,1], '', False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoPgmVYAMDea"
      },
      "source": [
        "### trening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS88SCbSMDeb"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModel, BertModel\n",
        "\n",
        "min_acc = 1000000\n",
        "min_test = []\n",
        "valid_preds = []\n",
        "test_preds = []\n",
        "best_model = False\n",
        "LR= 2e-5 \n",
        "\n",
        "gkf = GroupKFold(n_splits=5).split(X=training_set_copy.text, groups=training_set_copy.text)\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
        "    if fold not in range(running_folds):\n",
        "          continue\n",
        "    train_inputs = [(inputs[i][train_idx])[:training_sample_count] for i in range(len(inputs))]\n",
        "    train_outputs = (outputs[train_idx])[:training_sample_count]\n",
        "\n",
        "    valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
        "    valid_outputs = outputs[valid_idx]\n",
        "\n",
        "    print(np.array(train_inputs).shape, np.array(train_outputs).shape)\n",
        "\n",
        "    K.clear_session()\n",
        "    model = create_model(MAX_SEQUENCE_LENGTH)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    for xx in range(1):\n",
        "        print(xx)\n",
        "        model.fit(train_inputs, train_outputs, epochs=training_epochs, batch_size=16, verbose=1)\n",
        "        valid_preds.append(model.predict(valid_inputs))\n",
        "\n",
        "        acc = print_evaluation_metrics(np.array(valid_outputs), np.array(valid_preds[-1]), 'on #'+str(xx+1))\n",
        "        if acc < min_acc:\n",
        "              print('Accuracy: ', acc)\n",
        "              min_acc = acc\n",
        "              best_model = model\n",
        "        print(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBOgZxXSMDed"
      },
      "outputs": [],
      "source": [
        "best_model.summary()\n",
        "print('Accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxDwpfAcMDeg"
      },
      "outputs": [],
      "source": [
        "min_test = best_model.predict(test_inputs)\n",
        "print(len(min_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3hcVtcpMDeh"
      },
      "source": [
        "## Wyniki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoiwD7hkakfr"
      },
      "outputs": [],
      "source": [
        "print(min_test[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPX2i5ulMDeh"
      },
      "outputs": [],
      "source": [
        "result = testing_set.copy()\n",
        "result['pred'] = min_test\n",
        "for i in range(len(min_test)):\n",
        "    min_test[i] = min_test[i] * 4\n",
        "result['humor_rating'] = min_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9Gj88xScAfj"
      },
      "outputs": [],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD4ZrlF4MDej",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for split in np.arange(0.1, 0.80, 0.1).tolist():\n",
        "    result['is_humor'] = (result['pred'] > split)\n",
        "\n",
        "\n",
        "result.to_csv('result_10e.csv', index=False)\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRB1tnYvcUyU"
      },
      "source": [
        "```\n",
        "2 e\n",
        "0 \tIf you are Asian in the kitchen and African in... \t0.985082 \t3.940329 \tTrue\n",
        "1 \tWhy is there only a stairway to heaven but a h... \t0.918508 \t3.674032 \tTrue\n",
        "2 \tI once dated a girl with a twin People asked m... \t0.998696 \t3.994783 \tTrue\n",
        "3 \tThere are people out their happier with less t... \t0.058198 \t0.232790 \tFalse\n",
        "4 \tOne zebra says to the other, \"I'm going to che... \t0.951897 \t3.807587 \tTrue\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "10e\n",
        "0 \tIf you are Asian in the kitchen and African in... \t0.999979 \t3.999916 \tTrue\n",
        "1 \tWhy is there only a stairway to heaven but a h... \t0.993642 \t3.974567 \tTrue\n",
        "2 \tI once dated a girl with a twin People asked m... \t0.999971 \t3.999885 \tTrue\n",
        "3 \tThere are people out their happier with less t... \t0.000607 \t0.002430 \tFalse\n",
        "4 \tOne zebra says to the other, \"I'm going to che... \t0.996539 \t3.986156 \tTrue\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "INL zadanie3.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
